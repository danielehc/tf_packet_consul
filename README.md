
# Base Consul Cluster on Packet.net

Generatea an arm64 cluster in Packet.net and installs Consul 1.5.1 amr32 on it.

## vars

```
export token="<the_packet_token>"
export TF_VAR_token=$token

export projectid=<the_project_id>
export TF_VAR_projectid=$projectid

export prefix=daniele
export TF_VAR_prefix=$prefix
```

By default it tries to create 3 server and 3 clients.

It is possible to tune this by using `server_count` and `client_count` variables:

```
export server_count="desired amount of nodes"
export TF_VAR_server_count=$server_count

export client_count="desired amount of nodes"
export TF_VAR_client_count=$client_count
```

## run

```
terraform fmt

terraform init

terraform plan

terraform apply
```

## Info

Sometimes Packet.net does not have enough available resources to spin up 6 nodes so it spins olny a subset of them (usually 4 or 5). 

In that case it will throw the following errror:
```
vagrant@tf-cli:/vagrant/labs/tf_packet$ terraform apply -auto-approve
packet_ssh_key.key1: Refreshing state... [id=...]
packet_device.consul_server[1]: Refreshing state... [id=...]
packet_device.consul_server[2]: Refreshing state... [id=...]
packet_device.consul_client[2]: Refreshing state... [id=...]
packet_device.consul_client[1]: Refreshing state... [id=...]
packet_device.consul_client[0]: Creating...
packet_device.consul_server[0]: Creating...

Error: The facility ewr1 has no provisionable c2.large.arm servers matching your criteria

  on main.tf line 11, in resource "packet_device" "consul_server":
  11: resource "packet_device" "consul_server" {



Error: The facility ewr1 has no provisionable c2.large.arm servers matching your criteria

  on main.tf line 76, in resource "packet_device" "consul_client":
  76: resource "packet_device" "consul_client" {

```

Often after a few minutes resource become available so running again:

```
terraform plan

terraform apply
```

will generate the missing resources.

## Load Generator

The scenario contains a basic load generator that implements the following scenarios:

### Service Register

Every instance of the scenario will:

* Create a service definition and register the service with check
* Iterate until the main test is still running (using  ${LOCK_FILE})
    * IF the service is healthy > pick a random service and make it unhealthy (by removing the Service lock file)
    * IF the service is un-healthy > fix it (by adding the Service lock file back)


To run this scenario use the following syntax:

```
/path/to/test_generate_consul_traffic.sh [-f] -t <number of threads> -s service_creation
```

`-f` - Flush (optional) - Removes files generated during the test (does not deregister services from Consul yet)
`-t` - Number of processes that get generated by the test, each process will perform the steps listed above
`-s service_creation` - Scenario name


Example:
```
/path/to/test_generate_consul_traffic.sh -f -t 10 -s service_creation
```

To remove services afterward is possible to use the following command:

```
for i in `curl  -s --request GET http://127.0.0.1:8500/v1/catalog/services | jq . | grep - | tr -d '"' | tr -d ','`; do consul services deregister -id=$i; sleep 1; done		
```

### Service Monitoring and DNS

Every instance of the scenario will:

* Pick a random service to monitor
* Iterate until the main test is still running (using  ${LOCK_FILE})
    * IF the service is healthy > Use DNS interface to get IP of the service
    * IF the service is un-healthy > Wait until it becomes healthy then use DNS interface to get IP of the service

To run this scenario use the following syntax:

```
/path/to/test_generate_consul_traffic.sh [-f] -t <number of threads> -s service_monitoring
```

`-f` - Flush (optional) - Removes files generated during the test (does not deregister services from Consul yet)
`-t` - Number of processes that get generated by the test, each process will perform the steps listed above
`-s service_monitoring` - Scenario name

Example:
```
/path/to/test_generate_consul_traffic.sh -f -t 10 -s service_monitoring
```